{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOoRWS18vXRpFvz11Z5WBT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namratabiswas/Attention_in_Transformers/blob/main/sentence_embedding_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryo4_jJFAFvO"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import PyTorch for building and running the model, math for computing the positional encodings, and cosine_similarity to measure how close two sentence embeddings are."
      ],
      "metadata": {
        "id": "H-iSVE7vCCMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "ZdciPIQNA47j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers have no sense of word order inherently. Therefore POSITIONAL ENCODING encodes each position with a unique pattern using sine/cosine functions.(keep track of the order of words)"
      ],
      "metadata": {
        "id": "SDTm3kTZCXGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "2Ps3inY3BFSu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Encoder Model\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=2, dim_feedforward=256, max_len=100):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        src = src.permute(1, 0, 2)  # [seq_len, batch, d_model]\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = output.mean(dim=0)  # Global average pooling\n",
        "        return self.fc(output)\n"
      ],
      "metadata": {
        "id": "pOpaw90MBL52"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters Explained:\n",
        "\n",
        "vocab_size: Size of vocabulary\n",
        "\n",
        "d_model: Dimensionality of embeddings\n",
        "\n",
        "nhead: Number of attention heads\n",
        "\n",
        "num_layers: Stacked encoder layers\n",
        "\n",
        "dim_feedforward: Size of feed-forward network inside encoder\n",
        "\n",
        "max_len: Max sequence length for positional encoding"
      ],
      "metadata": {
        "id": "dmE8qn2XC8WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy Vocabulary and Tokenizer\n",
        "vocab = {'I': 0, 'like': 1, 'cats': 2, 'dogs': 3, 'hate': 4, 'you': 5, 'love': 6}\n",
        "def tokenize(sentence):\n",
        "    return [vocab[word] for word in sentence.split() if word in vocab]\n"
      ],
      "metadata": {
        "id": "qzSibTmiBQuZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Sentences\n",
        "sentences = [\n",
        "    (\"I like cats\", \"I like dogs\"),\n",
        "    (\"I love cats\", \"I hate cats\"),\n",
        "    (\"you like dogs\", \"I like cats\")\n",
        "]"
      ],
      "metadata": {
        "id": "HD2pfiKRBWl7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Padding and Tensors\n",
        "\n",
        "# Pads input to a fixed length\n",
        "def pad_sequence(seq, max_len, pad_value=0):\n",
        "    return seq + [pad_value] * (max_len - len(seq))\n",
        "\n",
        "# Max token length\n",
        "max_len = 5\n",
        "X = []\n",
        "\n",
        "# Convert all sentence pairs to tensors\n",
        "for s1, s2 in sentences:\n",
        "    tokens1 = pad_sequence(tokenize(s1), max_len)\n",
        "    tokens2 = pad_sequence(tokenize(s2), max_len)\n",
        "    X.append((torch.tensor(tokens1), torch.tensor(tokens2)))"
      ],
      "metadata": {
        "id": "7Yk0SSnaBbLV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and Run Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerEncoderModel(vocab_size=len(vocab), max_len=max_len).to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Sentence Similarity Results:\\n\")\n",
        "with torch.no_grad():\n",
        "    for (t1, t2), (s1, s2) in zip(X, sentences):\n",
        "        t1 = t1.unsqueeze(0).to(device)\n",
        "        t2 = t2.unsqueeze(0).to(device)\n",
        "        emb1 = model(t1).cpu().numpy()\n",
        "        emb2 = model(t2).cpu().numpy()\n",
        "        sim = cosine_similarity(emb1, emb2)[0][0]\n",
        "        print(f\"Similarity between: \\\"{s1}\\\" and \\\"{s2}\\\" → {sim:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcmBQaa8BgnC",
        "outputId": "c99540d0-deea-4f5c-881f-55498e3bf42c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Similarity Results:\n",
            "\n",
            "Similarity between: \"I like cats\" and \"I like dogs\" → 0.962\n",
            "Similarity between: \"I love cats\" and \"I hate cats\" → 0.947\n",
            "Similarity between: \"you like dogs\" and \"I like cats\" → 0.908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}