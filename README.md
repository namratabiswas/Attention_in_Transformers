# Attention_in_Transformers


Summary of Learning
Concepts we are Learning

Transformer Encoder -	How it works and why it's used for understanding inputs
Positional Encoding -	How order is encoded using sine/cosine functions
Sentence Embedding -	How encoder transforms input into a fixed-length vector
Cosine Similarity -	How to compute similarity between two sentence vectors
PyTorch TransformerEncoder -	How to build a custom Transformer using PyTorch's encoder stack



We will also use BERT creatively for masked language modeling (MLM) to “generate” text by filling in the blanks.

What we'll Learn:

How to use BERT for masked token generation

Visualize attention weights using bertviz

